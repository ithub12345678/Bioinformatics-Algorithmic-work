{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Part A: File Format Handling and Data Processing (35 marks)**"
      ],
      "metadata": {
        "id": "ZM-5KC_CQd1E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmJPsTOKP9Mn",
        "outputId": "56a6f452-5729-4f19-f732-836b42e32e41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "  Running Demonstration for Assignment 3 - Part A\n",
            "============================================================\n",
            "\n",
            "--- Testing Q1: FASTA Parsing ---\n",
            "[Info] Parsing FASTA file: demo_multi.fasta\n",
            "  Parsed Header: >seq1 K12 E. coli\n",
            "  Parsed Sequence: ATGCGTACGTACGATCAGTCAGTCAGACTG... (len: 122)\n",
            "  Parsed Header: >seq2 M. tuberculosis\n",
            "  Parsed Sequence: ATGGGGGGGGGGGGGGGGGGGGGCCCCCCC... (len: 99)\n",
            "  Parsed Header: >seq3_with_invalid_chars\n",
            "  Parsed Sequence: ATGCGTACGTACGXATCAGTCAGTZY... (len: 26)\n",
            "\n",
            "--- Testing Q1: FASTA Writing ---\n",
            "[Info] Writing sequences to FASTA file: demo_output.fasta\n",
            "[Success] FASTA file written to demo_output.fasta\n",
            "\n",
            "--- Testing Q2: Data Integration (SQLite) ---\n",
            "[Info] Parsing FASTA file: demo_multi.fasta\n",
            "[QC Fail] Invalid characters found in sequence: ATGCGTACGTACGXATCAGT...\n",
            "[Success] Loaded 2 records from FASTA into genomics_db.sqlite\n",
            "[Info] Parsing FASTQ file: demo.fastq\n",
            "[Success] Loaded 0 records from FASTQ into genomics_db.sqlite\n",
            "\n",
            "--- Testing Q2: Database Search ---\n",
            "Searching for 'seq%'...\n",
            "[Info] Searching for records like: 'seq%'\n",
            "  Found: ('seq1', '>seq1 K12 E. coli', 'FASTA')\n",
            "  Found: ('seq2', '>seq2 M. tuberculosis', 'FASTA')\n",
            "\n",
            "Searching for 'read%'...\n",
            "[Info] Searching for records like: 'read%'\n",
            "\n",
            "============================================================\n",
            "  Demonstration Complete.\n",
            "  Check files: 'genomics_db.sqlite' and 'demo_output.fasta'\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Solution for Assignment 3, Part A:\n",
        "File Format Handling and Data Processing.\n",
        "\n",
        "This script implements:\n",
        "- Q1: A robust, memory-efficient FASTA parser and writer.\n",
        "- Q2: A data integration system using SQLite, including\n",
        "      a FASTQ parser, data validation, and search functions.\n",
        "\"\"\"\n",
        "\n",
        "import sqlite3\n",
        "import re\n",
        "from typing import Iterator, Tuple, Dict, Any, List\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# --- Question 1: FASTA File Processing\n",
        "# -----------------------------------------------------------------\n",
        "\n",
        "class FastaHandler:\n",
        "    \"\"\"\n",
        "    Implements robust FASTA file parsing and writing.\n",
        "    \"\"\"\n",
        "\n",
        "    def parse_fasta(self, filepath: str) -> Iterator[Tuple[str, str]]:\n",
        "        \"\"\"\n",
        "        Parses a FASTA file with multiple sequences efficiently[cite: 26, 28].\n",
        "\n",
        "        Uses a generator to yield one sequence at a time, keeping\n",
        "        memory usage low. It correctly handles multi-line sequences.\n",
        "\n",
        "        Args:\n",
        "            filepath: The path to the FASTA file.\n",
        "\n",
        "        Yields:\n",
        "            A tuple of (header, sequence) for each record in the file.\n",
        "            - header: The full header line (e.g., '>seq1 description') [cite: 27]\n",
        "            - sequence: The full DNA/protein sequence as a single string.\n",
        "        \"\"\"\n",
        "        print(f\"[Info] Parsing FASTA file: {filepath}\")\n",
        "        try:\n",
        "            with open(filepath, 'r') as f:\n",
        "                header = None\n",
        "                sequence_lines = []\n",
        "\n",
        "                for line in f:\n",
        "                    line = line.strip()\n",
        "                    if not line:\n",
        "                        continue  # Skip empty lines\n",
        "\n",
        "                    if line.startswith('>'):\n",
        "                        # If we have a sequence buffered, yield it\n",
        "                        if header:\n",
        "                            yield header, \"\".join(sequence_lines)\n",
        "\n",
        "                        # Start a new sequence\n",
        "                        header = line\n",
        "                        sequence_lines = []\n",
        "                    else:\n",
        "                        # Append sequence data\n",
        "                        if header:\n",
        "                            sequence_lines.append(line)\n",
        "\n",
        "                # Yield the last sequence in the file\n",
        "                if header:\n",
        "                    yield header, \"\".join(sequence_lines)\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"[Error] File not found: {filepath}\")\n",
        "        except Exception as e:\n",
        "            print(f\"[Error] Failed to parse FASTA: {e}\")\n",
        "\n",
        "    def write_fasta(self, filepath: str,\n",
        "                    sequences: Iterator[Tuple[str, str]],\n",
        "                    line_width: int = 60) -> None:\n",
        "        \"\"\"\n",
        "        Implements FASTA writing capabilities with proper formatting.\n",
        "\n",
        "        Args:\n",
        "            filepath: The path to the output FASTA file.\n",
        "            sequences: An iterator (like from parse_fasta) that provides\n",
        "                       (header, sequence) tuples.\n",
        "            line_width: The maximum number of characters per sequence line.\n",
        "        \"\"\"\n",
        "        print(f\"[Info] Writing sequences to FASTA file: {filepath}\")\n",
        "        try:\n",
        "            with open(filepath, 'w') as f:\n",
        "                for header, sequence in sequences:\n",
        "                    # Ensure header starts with '>'\n",
        "                    if not header.startswith('>'):\n",
        "                        header = '>' + header\n",
        "\n",
        "                    f.write(f\"{header}\\n\")\n",
        "\n",
        "                    # Write sequence in formatted lines\n",
        "                    for i in range(0, len(sequence), line_width):\n",
        "                        f.write(f\"{sequence[i:i + line_width]}\\n\")\n",
        "            print(f\"[Success] FASTA file written to {filepath}\")\n",
        "        except Exception as e:\n",
        "            print(f\"[Error] Failed to write FASTA: {e}\")\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# --- Question 2: Genomic Data Integration\n",
        "# -----------------------------------------------------------------\n",
        "\n",
        "class GenomicDataHandler:\n",
        "    \"\"\"\n",
        "    Develops data management systems for genomic data.\n",
        "\n",
        "    Uses SQLite for database storage  and includes\n",
        "    parsers for FASTA/FASTQ , data validation,\n",
        "    and search functions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, db_path: str):\n",
        "        \"\"\"\n",
        "        Initializes the handler with a path to the SQLite database.\n",
        "\n",
        "        Args:\n",
        "            db_path: Path to the .sqlite file (e.g., 'genomics.db')\n",
        "        \"\"\"\n",
        "        self.db_path = db_path\n",
        "        self._create_database()\n",
        "\n",
        "    def _create_database(self) -> None:\n",
        "        \"\"\"\n",
        "        Creates the database tables if they don't already exist.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            with sqlite3.connect(self.db_path) as conn:\n",
        "                cursor = conn.cursor()\n",
        "                cursor.execute('''\n",
        "                CREATE TABLE IF NOT EXISTS sequences (\n",
        "                    id TEXT PRIMARY KEY,\n",
        "                    header TEXT NOT NULL,\n",
        "                    sequence TEXT NOT NULL,\n",
        "                    source_format TEXT NOT NULL,\n",
        "                    metadata_json TEXT\n",
        "                )\n",
        "                ''')\n",
        "                conn.commit()\n",
        "        except sqlite3.Error as e:\n",
        "            print(f\"[Error] Database creation failed: {e}\")\n",
        "\n",
        "    def validate_sequence(self, sequence: str,\n",
        "                          alphabet: str = r'^[ATCGN]+$') -> bool:\n",
        "        \"\"\"\n",
        "        Builds data validation and quality control measures.\n",
        "\n",
        "        Checks if a sequence contains only valid characters (A, T, C, G, N).\n",
        "\n",
        "        Args:\n",
        "            sequence: The DNA sequence string to check.\n",
        "            alphabet: A regex for the valid character set.\n",
        "\n",
        "        Returns:\n",
        "            True if valid, False otherwise.\n",
        "        \"\"\"\n",
        "        if re.match(alphabet, sequence.upper()):\n",
        "            return True\n",
        "        print(f\"[QC Fail] Invalid characters found in sequence: {sequence[:20]}...\")\n",
        "        return False\n",
        "\n",
        "    def parse_fastq(self, filepath: str) -> Iterator[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Handles the basics of the FASTQ file format.\n",
        "\n",
        "        Uses a generator to yield one record at a time, keeping\n",
        "        memory usage low.\n",
        "\n",
        "        Args:\n",
        "            filepath: The path to the FASTQ file.\n",
        "\n",
        "        Yields:\n",
        "            A dictionary for each record:\n",
        "            {'header': str, 'sequence': str, 'quality': str}\n",
        "        \"\"\"\n",
        "        print(f\"[Info] Parsing FASTQ file: {filepath}\")\n",
        "        try:\n",
        "            with open(filepath, 'r') as f:\n",
        "                while True:\n",
        "                    # 1. Header line\n",
        "                    header = f.readline().strip()\n",
        "                    if not header:\n",
        "                        break  # End of file\n",
        "\n",
        "                    # 2. Sequence line\n",
        "                    sequence = f.readline().strip()\n",
        "\n",
        "                    # 3. Plus line\n",
        "                    plus_line = f.readline().strip()\n",
        "\n",
        "                    # 4. Quality line\n",
        "                    quality = f.readline().strip()\n",
        "\n",
        "                    if not (header.startswith('@') and plus_line == '+' and \\\n",
        "                            len(sequence) == len(quality)):\n",
        "                        print(f\"[Error] Corrupt FASTQ record found. Skipping.\")\n",
        "                        continue\n",
        "\n",
        "                    yield {\n",
        "                        'header': header,\n",
        "                        'sequence': sequence,\n",
        "                        'quality': quality\n",
        "                    }\n",
        "        except FileNotFoundError:\n",
        "            print(f\"[Error] File not found: {filepath}\")\n",
        "        except Exception as e:\n",
        "            print(f\"[Error] Failed to parse FASTQ: {e}\")\n",
        "\n",
        "    def load_data_to_db(self, parser_gen: Iterator,\n",
        "                        source_format: str) -> None:\n",
        "        \"\"\"\n",
        "        Loads data from a parser (FASTA or FASTQ) into the database.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            with sqlite3.connect(self.db_path) as conn:\n",
        "                cursor = conn.cursor()\n",
        "                count = 0\n",
        "                for record in parser_gen:\n",
        "                    if source_format == 'FASTA':\n",
        "                        header, seq = record\n",
        "                        record_id = header.split()[0][1:] # Get '>id' part\n",
        "                        metadata = {'full_header': header}\n",
        "                    elif source_format == 'FASTQ':\n",
        "                        header = record['header']\n",
        "                        seq = record['sequence']\n",
        "                        record_id = header.split()[0][1:] # Get '@id' part\n",
        "                        metadata = {'quality_scores': record['quality']}\n",
        "\n",
        "                    # Run validation\n",
        "                    if not self.validate_sequence(seq):\n",
        "                        continue\n",
        "\n",
        "                    # Insert data, ignore if ID already exists\n",
        "                    cursor.execute(\n",
        "                        \"INSERT OR IGNORE INTO sequences (id, header, sequence, source_format, metadata_json) VALUES (?, ?, ?, ?, ?)\",\n",
        "                        (record_id, header, seq, source_format, str(metadata))\n",
        "                    )\n",
        "                    count += 1\n",
        "                conn.commit()\n",
        "                print(f\"[Success] Loaded {count} records from {source_format} into {self.db_path}\")\n",
        "        except sqlite3.Error as e:\n",
        "            print(f\"[Error] Failed to load data to database: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"[Error] An unexpected error occurred during DB load: {e}\")\n",
        "\n",
        "    def search_by_id(self, seq_id_pattern: str) -> List[Tuple]:\n",
        "        \"\"\"\n",
        "        Implements search and retrieval functions.\n",
        "\n",
        "        Searches for records where the ID matches a pattern (e.g., 'seq1%').\n",
        "\n",
        "        Args:\n",
        "            seq_id_pattern: An SQL LIKE pattern (e.g., 'Ecoli_gene%').\n",
        "\n",
        "        Returns:\n",
        "            A list of matching records (id, header, source_format).\n",
        "        \"\"\"\n",
        "        print(f\"[Info] Searching for records like: '{seq_id_pattern}'\")\n",
        "        try:\n",
        "            with sqlite3.connect(self.db_path) as conn:\n",
        "                cursor = conn.cursor()\n",
        "                cursor.execute(\n",
        "                    \"SELECT id, header, source_format FROM sequences WHERE id LIKE ?\",\n",
        "                    (seq_id_pattern,)\n",
        "                )\n",
        "                results = cursor.fetchall()\n",
        "                return results\n",
        "        except sqlite3.Error as e:\n",
        "            print(f\"[Error] Database search failed: {e}\")\n",
        "            return []\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# --- Demonstration & Testing [cite: 6]\n",
        "# -----------------------------------------------------------------\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\"*60)\n",
        "    print(\"  Running Demonstration for Assignment 3 - Part A\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # --- Setup: Create dummy files for testing ---\n",
        "\n",
        "    # Dummy FASTA data\n",
        "    fasta_content = \"\"\"\n",
        ">seq1 K12 E. coli\n",
        "ATGCGTACGTACGATCAGTCAGTCAGACTGACATCAGTACGATCAGTCA\n",
        "GTCAGACTGACATCAGTACGATCAGTCAGTCAGACTGACATCAGTACGA\n",
        "TCAGTCAGTCAGACTGACATCAGT\n",
        ">seq2 M. tuberculosis\n",
        "ATGGGGGGGGGGGGGGGGGGGGGCCCCCCCCCCCCCCCCCCCTATATATA\n",
        "TATATATATATATATAGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\n",
        ">seq3_with_invalid_chars\n",
        "ATGCGTACGTACGXATCAGTCAGTZY\n",
        "\"\"\"\n",
        "    with open(\"demo_multi.fasta\", \"w\") as f:\n",
        "        f.write(fasta_content)\n",
        "\n",
        "    # Dummy FASTQ data\n",
        "    fastq_content = \"\"\"\n",
        "@read1_flowcell_1\n",
        "ATGCGTACGTACGATCAGTCAGTCAGACTGACATCAGTACGATCAGTCAG\n",
        "+\n",
        "IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII\n",
        "@read2_flowcell_1\n",
        "ATGGGGGGGGGGGGGGGGGGGGGCCCCCCCCCCCCCCCCCCCTATATATA\n",
        "+\n",
        "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "\"\"\"\n",
        "    with open(\"demo.fastq\", \"w\") as f:\n",
        "        f.write(fastq_content)\n",
        "\n",
        "\n",
        "    # --- Q1 Test: FASTA Parsing [cite: 26, 27, 28] ---\n",
        "    print(\"\\n--- Testing Q1: FASTA Parsing ---\")\n",
        "    fasta_handler = FastaHandler()\n",
        "    fasta_gen = fasta_handler.parse_fasta(\"demo_multi.fasta\")\n",
        "\n",
        "    # Store parsed data for writing and DB loading\n",
        "    parsed_sequences = list(fasta_gen)\n",
        "\n",
        "    for header, seq in parsed_sequences:\n",
        "        print(f\"  Parsed Header: {header}\")\n",
        "        print(f\"  Parsed Sequence: {seq[:30]}... (len: {len(seq)})\")\n",
        "\n",
        "    # --- Q1 Test: FASTA Writing  ---\n",
        "    print(\"\\n--- Testing Q1: FASTA Writing ---\")\n",
        "    # We'll write the non-invalid sequences back to a new file\n",
        "    sequences_to_write = [\n",
        "        (h, s) for h, s in parsed_sequences if \"invalid\" not in h\n",
        "    ]\n",
        "    fasta_handler.write_fasta(\"demo_output.fasta\", sequences_to_write)\n",
        "\n",
        "\n",
        "    # --- Q2 Test: Data Integration & Validation [cite: 31, 33, 34] ---\n",
        "    print(\"\\n--- Testing Q2: Data Integration (SQLite) ---\")\n",
        "    db_handler = GenomicDataHandler(\"genomics_db.sqlite\")\n",
        "\n",
        "    # Load FASTA data (note: 'seq3' will fail validation )\n",
        "    db_handler.load_data_to_db(\n",
        "        fasta_handler.parse_fasta(\"demo_multi.fasta\"),\n",
        "        \"FASTA\"\n",
        "    )\n",
        "\n",
        "    # Load FASTQ data\n",
        "    db_handler.load_data_to_db(\n",
        "        db_handler.parse_fastq(\"demo.fastq\"),\n",
        "        \"FASTQ\"\n",
        "    )\n",
        "\n",
        "    # --- Q2 Test: Search & Retrieval  ---\n",
        "    print(\"\\n--- Testing Q2: Database Search ---\")\n",
        "\n",
        "    print(\"Searching for 'seq%'...\")\n",
        "    seq_results = db_handler.search_by_id(\"seq%\")\n",
        "    for res in seq_results:\n",
        "        print(f\"  Found: {res}\")\n",
        "\n",
        "    print(\"\\nSearching for 'read%'...\")\n",
        "    read_results = db_handler.search_by_id(\"read%\")\n",
        "    for res in read_results:\n",
        "        print(f\"  Found: {res}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"  Demonstration Complete.\")\n",
        "    print(\"  Check files: 'genomics_db.sqlite' and 'demo_output.fasta'\")\n",
        "    print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part B: Rosalind-Style Problem Solving (40 marks)**"
      ],
      "metadata": {
        "id": "1TWLR3x0SGGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Solution for Assignment 3, Part B:\n",
        "Rosalind-Style Problem Solving.\n",
        "\n",
        "This script implements a wide range of core bioinformatics algorithms\n",
        "for sequence comparison and pattern analysis.\n",
        "\n",
        "- Q3: Needleman-Wunsch, LCS, p-distance, Jukes-Cantor, Consensus\n",
        "- Q4: Suffix Array, Repeats/Palindromes, Greedy Assembly, UPGMA\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "from collections import Counter\n",
        "import numpy as np  # Required for alignment matrices [cite: 15]\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# --- Question 3: Multiple Sequence Problems\n",
        "# -----------------------------------------------------------------\n",
        "\n",
        "def align_global_needleman_wunsch(seq1: str, seq2: str,\n",
        "                                  match_score: int = 1,\n",
        "                                  mismatch_penalty: int = -1,\n",
        "                                  gap_penalty: int = -2) -> (int, str, str):\n",
        "    \"\"\"\n",
        "    Implements global sequence alignment basics using Needleman-Wunsch.\n",
        "\n",
        "    Uses dynamic programming to find the optimal global alignment.\n",
        "\n",
        "    Time Complexity: O(n*m)\n",
        "    Space Complexity: O(n*m)\n",
        "\n",
        "    Args:\n",
        "        seq1 (str): First sequence\n",
        "        seq2 (str): Second sequence\n",
        "        match_score (int): Score for a match\n",
        "        mismatch_penalty (int): Penalty for a mismatch (negative)\n",
        "        gap_penalty (int): Penalty for a gap (negative)\n",
        "\n",
        "    Returns:\n",
        "        A tuple: (alignment_score, aligned_seq1, aligned_seq2)\n",
        "    \"\"\"\n",
        "    n = len(seq1)\n",
        "    m = len(seq2)\n",
        "\n",
        "    # Initialize the score matrix\n",
        "    # We use numpy for efficient matrix operations [cite: 15]\n",
        "    score_matrix = np.zeros((n + 1, m + 1))\n",
        "\n",
        "    # Initialize the traceback matrix\n",
        "    # 0 = diagonal, 1 = up, 2 = left\n",
        "    trace_matrix = np.zeros((n + 1, m + 1), dtype=int)\n",
        "\n",
        "    # Fill the first row and column with gap penalties\n",
        "    for i in range(n + 1):\n",
        "        score_matrix[i, 0] = i * gap_penalty\n",
        "        trace_matrix[i, 0] = 1  # From 'up'\n",
        "    for j in range(m + 1):\n",
        "        score_matrix[0, j] = j * gap_penalty\n",
        "        trace_matrix[0, j] = 2  # From 'left'\n",
        "\n",
        "    # Fill the DP matrix\n",
        "    for i in range(1, n + 1):\n",
        "        for j in range(1, m + 1):\n",
        "            # Calculate score for match/mismatch\n",
        "            match = (match_score if seq1[i - 1] == seq2[j - 1]\n",
        "                     else mismatch_penalty)\n",
        "\n",
        "            diag_score = score_matrix[i - 1, j - 1] + match\n",
        "            up_score = score_matrix[i - 1, j] + gap_penalty\n",
        "            left_score = score_matrix[i, j - 1] + gap_penalty\n",
        "\n",
        "            scores = [diag_score, up_score, left_score]\n",
        "            score_matrix[i, j] = max(scores)\n",
        "            trace_matrix[i, j] = np.argmax(scores) # 0, 1, or 2\n",
        "\n",
        "    # --- Traceback ---\n",
        "    align1, align2 = \"\", \"\"\n",
        "    i, j = n, m\n",
        "    final_score = score_matrix[i, j]\n",
        "\n",
        "    while i > 0 or j > 0:\n",
        "        trace = trace_matrix[i, j]\n",
        "        if trace == 0:  # Diagonal\n",
        "            align1 = seq1[i - 1] + align1\n",
        "            align2 = seq2[j - 1] + align2\n",
        "            i -= 1\n",
        "            j -= 1\n",
        "        elif trace == 1:  # Up\n",
        "            align1 = seq1[i - 1] + align1\n",
        "            align2 = \"-\" + align2\n",
        "            i -= 1\n",
        "        else:  # Left\n",
        "            align1 = \"-\" + align1\n",
        "            align2 = seq2[j - 1] + align2\n",
        "            j -= 1\n",
        "\n",
        "    return final_score, align1, align2\n",
        "\n",
        "def find_longest_common_subsequence(seq1: str, seq2: str) -> (int, str):\n",
        "    \"\"\"\n",
        "    Finds the longest common subsequence between two sequences.\n",
        "    This is different from a sub*string*, as a sub*sequence*\n",
        "    does not have to be contiguous.\n",
        "\n",
        "    Time Complexity: O(n*m)\n",
        "    Space Complexity: O(n*m)\n",
        "    \"\"\"\n",
        "    n = len(seq1)\n",
        "    m = len(seq2)\n",
        "\n",
        "    # dp_table[i][j] will store the length of the LCS of\n",
        "    # seq1[0...i-1] and seq2[0...j-1]\n",
        "    dp_table = np.zeros((n + 1, m + 1), dtype=int)\n",
        "\n",
        "    # Fill the table\n",
        "    for i in range(1, n + 1):\n",
        "        for j in range(1, m + 1):\n",
        "            if seq1[i - 1] == seq2[j - 1]:\n",
        "                dp_table[i, j] = dp_table[i - 1, j - 1] + 1\n",
        "            else:\n",
        "                dp_table[i, j] = max(dp_table[i - 1, j], dp_table[i, j - 1])\n",
        "\n",
        "    # --- Traceback to find the subsequence ---\n",
        "    lcs_str = \"\"\n",
        "    i, j = n, m\n",
        "    while i > 0 and j > 0:\n",
        "        if seq1[i - 1] == seq2[j - 1]:\n",
        "            lcs_str = seq1[i - 1] + lcs_str\n",
        "            i -= 1\n",
        "            j -= 1\n",
        "        elif dp_table[i - 1, j] > dp_table[i, j - 1]:\n",
        "            i -= 1\n",
        "        else:\n",
        "            j -= 1\n",
        "\n",
        "    return dp_table[n, m], lcs_str\n",
        "\n",
        "def calc_p_distance(seq1: str, seq2: str) -> float:\n",
        "    \"\"\"\n",
        "    Calculates evolutionary distance using a simple model: p-distance.\n",
        "    p-distance is the proportion of sites at which the\n",
        "    two sequences differ. Assumes sequences are aligned.\n",
        "\n",
        "    Time Complexity: O(n)\n",
        "    Space Complexity: O(1)\n",
        "    \"\"\"\n",
        "    if len(seq1) != len(seq2):\n",
        "        raise ValueError(\"Sequences must be aligned (equal length).\")\n",
        "\n",
        "    n = len(seq1)\n",
        "    differences = 0\n",
        "    for i in range(n):\n",
        "        if seq1[i] != seq2[i] and seq1[i] != '-' and seq2[i] != '-':\n",
        "            differences += 1\n",
        "\n",
        "    return differences / n\n",
        "\n",
        "def calc_jukes_cantor_distance(p_distance: float) -> float:\n",
        "    \"\"\"\n",
        "    Calculates evolutionary distance using the Jukes-Cantor (JC69) model.\n",
        "    It corrects the p-distance for multiple substitutions at the same site.\n",
        "\n",
        "    Time Complexity: O(1)\n",
        "    Space Complexity: O(1)\n",
        "    \"\"\"\n",
        "    if p_distance >= 0.75:\n",
        "        return float('inf')  # Distance is undefined\n",
        "    return - (3 / 4) * math.log(1 - (4 / 3) * p_distance)\n",
        "\n",
        "def generate_consensus_from_alignment(aligned_seqs: list[str]) -> (str, dict):\n",
        "    \"\"\"\n",
        "    Develops consensus sequence generation algorithms.\n",
        "    Takes a list of *already aligned* sequences.\n",
        "\n",
        "    Time Complexity: O(n*m) where n=num_seqs, m=seq_length\n",
        "    Space Complexity: O(m*k) where m=seq_length, k=alphabet_size\n",
        "\n",
        "    Args:\n",
        "        aligned_seqs: A list of aligned sequences (e.g., [\"A-T\", \"AAT\", \"A-C\"])\n",
        "\n",
        "    Returns:\n",
        "        A tuple: (consensus_sequence, profile_matrix)\n",
        "        - profile_matrix is a dict of {base: [counts]}\n",
        "    \"\"\"\n",
        "    if not aligned_seqs:\n",
        "        return \"\", {}\n",
        "\n",
        "    seq_length = len(aligned_seqs[0])\n",
        "    consensus = \"\"\n",
        "    profile = {base: [0] * seq_length for base in \"ACGT-\"}\n",
        "\n",
        "    for j in range(seq_length):  # For each position\n",
        "        col_counts = Counter()\n",
        "        for i in range(len(aligned_seqs)):  # For each sequence\n",
        "            base = aligned_seqs[i][j].upper()\n",
        "            if base in profile:\n",
        "                profile[base][j] += 1\n",
        "                col_counts[base] += 1\n",
        "\n",
        "        # Determine consensus base (ignore gaps unless they are majority)\n",
        "        non_gap_counts = {b: c for b, c in col_counts.items() if b != '-'}\n",
        "        if non_gap_counts:\n",
        "            consensus_base = max(non_gap_counts, key=non_gap_counts.get)\n",
        "        else:\n",
        "            consensus_base = '-'  # Only if column is all gaps\n",
        "        consensus += consensus_base\n",
        "\n",
        "    return consensus, profile\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# --- Question 4: Advanced Pattern Analysis\n",
        "# -----------------------------------------------------------------\n",
        "\n",
        "def build_suffix_array(text: str) -> list[int]:\n",
        "    \"\"\"\n",
        "    Implements suffix array concepts for pattern matching.\n",
        "    This is a simple O(n^2 log n) build, good for demonstration.\n",
        "    More advanced builds are O(n log n) or O(n).\n",
        "\n",
        "    Time Complexity: O(n^2 log n) - n suffixes, O(n) for slicing,\n",
        "                     O(n log n) for sorting n items.\n",
        "                     (Note: Python's sort is highly optimized,\n",
        "                     but string comparison can take O(n) in worst case)\n",
        "    Space Complexity: O(n^2) - to store all suffixes temporarily\n",
        "\n",
        "    Args:\n",
        "        text: The input text (e.g., a genome)\n",
        "\n",
        "    Returns:\n",
        "        List of integers (the suffix array)\n",
        "    \"\"\"\n",
        "    text += '$'  # Add terminal character\n",
        "    n = len(text)\n",
        "\n",
        "    # Generate all suffixes\n",
        "    suffixes = []\n",
        "    for i in range(n):\n",
        "        suffixes.append((text[i:], i)) # (suffix, original_index)\n",
        "\n",
        "    # Sort suffixes lexicographically\n",
        "    suffixes.sort()\n",
        "\n",
        "    # The suffix array is just the list of original indices\n",
        "    suffix_array = [index for suffix, index in suffixes]\n",
        "    return suffix_array\n",
        "\n",
        "def find_pattern_suffix_array(text: str, pattern: str, sa: list[int]) -> list[int]:\n",
        "    \"\"\"\n",
        "    Finds a pattern using a pre-computed Suffix Array (SA).\n",
        "    Uses binary search for efficient O(m log n) lookup.\n",
        "\n",
        "    Time Complexity: O(m * log n) - m=pattern len, n=text len\n",
        "    Space Complexity: O(1)\n",
        "\n",
        "    Args:\n",
        "        text (str): The *original* text (must end in '$')\n",
        "        pattern (str): The pattern to find\n",
        "        sa (list[int]): The pre-computed suffix array\n",
        "\n",
        "    Returns:\n",
        "        List of start indices where pattern occurs.\n",
        "    \"\"\"\n",
        "    n = len(text)\n",
        "    m = len(pattern)\n",
        "\n",
        "    # Binary search for the *first* occurrence\n",
        "    low, high = 0, n - 1\n",
        "    first_hit = -1\n",
        "    while low <= high:\n",
        "        mid = (low + high) // 2\n",
        "        suffix_start = sa[mid]\n",
        "        # Compare pattern to the suffix\n",
        "        suffix = text[suffix_start : suffix_start + m]\n",
        "\n",
        "        if pattern == suffix:\n",
        "            first_hit = mid\n",
        "            high = mid - 1  # Keep looking left\n",
        "        elif pattern < suffix:\n",
        "            high = mid - 1\n",
        "        else:\n",
        "            low = mid + 1\n",
        "\n",
        "    if first_hit == -1:\n",
        "        return []  # Pattern not found\n",
        "\n",
        "    # Binary search for the *last* occurrence\n",
        "    low, high = first_hit, n - 1\n",
        "    last_hit = first_hit\n",
        "    while low <= high:\n",
        "        mid = (low + high) // 2\n",
        "        suffix_start = sa[mid]\n",
        "        suffix = text[suffix_start : suffix_start + m]\n",
        "\n",
        "        if pattern == suffix:\n",
        "            last_hit = mid\n",
        "            low = mid + 1 # Keep looking right\n",
        "        elif pattern < suffix:\n",
        "            high = mid - 1\n",
        "        else:\n",
        "            low = mid + 1\n",
        "\n",
        "    # All indices from first_hit to last_hit in the SA are matches\n",
        "    return sorted([sa[i] for i in range(first_hit, last_hit + 1)])\n",
        "\n",
        "def find_reverse_palindromes(seq: str, min_len: int = 4, max_len: int = 12) -> list:\n",
        "    \"\"\"\n",
        "    Develops algorithms for finding repeats and palindromes.\n",
        "    This finds \"reverse palindromes\" (e.g., 'GAATTC' -> 'CTTAAG').\n",
        "    These are common in biology (e.g., restriction sites).\n",
        "\n",
        "    Time Complexity: O(n * k) where n=len(seq), k=max_len\n",
        "    Space Complexity: O(1)\n",
        "    \"\"\"\n",
        "    def reverse_complement(s):\n",
        "        comp_map = str.maketrans('ATCG', 'TAGC')\n",
        "        return s.upper().translate(comp_map)[::-1]\n",
        "\n",
        "    results = []\n",
        "    n = len(seq)\n",
        "    for length in range(min_len, max_len + 1):\n",
        "        for i in range(n - length + 1):\n",
        "            substring = seq[i : i + length]\n",
        "            if substring == reverse_complement(substring):\n",
        "                results.append((i, length, substring))\n",
        "    return results\n",
        "\n",
        "def simulate_greedy_assembly(reads: list[str], min_overlap: int = 5) -> str:\n",
        "    \"\"\"\n",
        "    Creates tools for sequence assembly simulation.\n",
        "    This implements a simple greedy assembler based on\n",
        "    the Overlap-Layout-Consensus (OLC) paradigm.\n",
        "\n",
        "    Time Complexity: O(k^2 * n) in each step, O(k^3 * n) overall?\n",
        "                     (k=num_reads, n=read_len). This is a rough\n",
        "                     estimate for this simple greedy approach.\n",
        "    Space Complexity: O(k*n)\n",
        "\n",
        "    Args:\n",
        "        reads (list[str]): A list of \"read\" sequences.\n",
        "        min_overlap (int): The minimum overlap to consider.\n",
        "\n",
        "    Returns:\n",
        "        The assembled contig (str).\n",
        "    \"\"\"\n",
        "\n",
        "    def find_best_overlap(seq1: str, seq2: str) -> (int, int):\n",
        "        \"\"\"Finds the best overlap (suffix of seq1 vs prefix of seq2).\"\"\"\n",
        "        best_overlap = 0\n",
        "\n",
        "        # Check for overlap: seq1_suffix == seq2_prefix\n",
        "        for k in range(min(len(seq1), len(seq2)), min_overlap - 1, -1):\n",
        "            if seq1.endswith(seq2[:k]):\n",
        "                best_overlap = k\n",
        "                break\n",
        "        return best_overlap\n",
        "\n",
        "    # Make a mutable copy of the reads\n",
        "    contigs = set(reads)\n",
        "\n",
        "    while len(contigs) > 1:\n",
        "        best_overlap = -1\n",
        "        best_pair = (None, None)\n",
        "        best_merge_type = 0 # 0: a->b, 1: b->a\n",
        "\n",
        "        # This is the O(k^2) part\n",
        "        read_list = list(contigs)\n",
        "        for i in range(len(read_list)):\n",
        "            for j in range(len(read_list)):\n",
        "                if i == j:\n",
        "                    continue\n",
        "\n",
        "                a = read_list[i]\n",
        "                b = read_list[j]\n",
        "\n",
        "                # Check a -> b overlap\n",
        "                overlap_ab = find_best_overlap(a, b)\n",
        "                if overlap_ab > best_overlap:\n",
        "                    best_overlap = overlap_ab\n",
        "                    best_pair = (a, b)\n",
        "                    best_merge_type = 0\n",
        "\n",
        "                # Check b -> a overlap\n",
        "                overlap_ba = find_best_overlap(b, a)\n",
        "                if overlap_ba > best_overlap:\n",
        "                    best_overlap = overlap_ba\n",
        "                    best_pair = (b, a)\n",
        "                    best_merge_type = 1\n",
        "\n",
        "        # If no overlap is found, we're stuck.\n",
        "        if best_overlap < min_overlap:\n",
        "            break\n",
        "\n",
        "        # Perform the merge\n",
        "        a, b = best_pair\n",
        "        contigs.remove(a)\n",
        "        contigs.remove(b)\n",
        "\n",
        "        # Merge: a_prefix + b\n",
        "        merged_contig = a + b[best_overlap:]\n",
        "        contigs.add(merged_contig)\n",
        "\n",
        "    # Return the largest remaining contig\n",
        "    return max(contigs, key=len)\n",
        "\n",
        "def build_upgma_tree(dist_matrix: dict, labels: list[str]) -> str:\n",
        "    \"\"\"\n",
        "    Builds phylogenetic relationship analysis tools.\n",
        "    Implements UPGMA (Unweighted Pair Group Method with Arithmetic Mean).\n",
        "\n",
        "    Time Complexity: O(n^3) - n clusters, O(n^2) to find min,\n",
        "                     O(n) to update matrix, n-1 merges.\n",
        "    Space Complexity: O(n^2) - to store the distance matrix.\n",
        "\n",
        "    Args:\n",
        "        dist_matrix (dict): A nested dict: {label1: {label2: dist}}\n",
        "        labels (list[str]): A list of species labels.\n",
        "\n",
        "    Returns:\n",
        "        A string representing the tree in Newick format.\n",
        "    \"\"\"\n",
        "\n",
        "    # Make a deep copy to work with\n",
        "    clusters = {label: {l: dist_matrix[label][l] for l in labels if l != label}\n",
        "                for label in labels}\n",
        "\n",
        "    # Store cluster info (tree topology and branch lengths)\n",
        "    # Start with each leaf as a cluster\n",
        "    cluster_info = {label: {'newick': label, 'size': 1, 'height': 0.0}\n",
        "                    for label in labels}\n",
        "\n",
        "    while len(clusters) > 1:\n",
        "        # Find the pair of clusters with the minimum distance\n",
        "        min_dist = float('inf')\n",
        "        c1_key, c2_key = None, None\n",
        "\n",
        "        c_keys = list(clusters.keys())\n",
        "        for i in range(len(c_keys)):\n",
        "            for j in range(i + 1, len(c_keys)):\n",
        "                k1, k2 = c_keys[i], c_keys[j]\n",
        "                if clusters[k1][k2] < min_dist:\n",
        "                    min_dist = clusters[k1][k2]\n",
        "                    c1_key, c2_key = k1, k2\n",
        "\n",
        "        # Merge the two clusters (c1 and c2) into a new cluster (c_new)\n",
        "        c1_info = cluster_info[c1_key]\n",
        "        c2_info = cluster_info[c2_key]\n",
        "\n",
        "        c_new_key = f\"({c1_key},{c2_key})\"\n",
        "        c_new_size = c1_info['size'] + c2_info['size']\n",
        "        c_new_height = min_dist / 2.0\n",
        "\n",
        "        # Create Newick string with branch lengths\n",
        "        c1_branch = c_new_height - c1_info['height']\n",
        "        c2_branch = c_new_height - c2_info['height']\n",
        "        c_new_newick = f\"({c1_info['newick']}:{c1_branch:.4f},{c2_info['newick']}:{c2_branch:.4f})\"\n",
        "\n",
        "        # Store info for the new cluster\n",
        "        cluster_info[c_new_key] = {\n",
        "            'newick': c_new_newick,\n",
        "            'size': c_new_size,\n",
        "            'height': c_new_height\n",
        "        }\n",
        "\n",
        "        # Calculate distances from the new cluster to all other clusters\n",
        "        new_row = {}\n",
        "        for other_key in clusters:\n",
        "            if other_key == c1_key or other_key == c2_key:\n",
        "                continue\n",
        "\n",
        "            # UPGMA: Arithmetic mean\n",
        "            dist = (clusters[c1_key][other_key] * c1_info['size'] +\n",
        "                    clusters[c2_key][other_key] * c2_info['size']) / c_new_size\n",
        "            new_row[other_key] = dist\n",
        "\n",
        "        # Remove old clusters from matrix\n",
        "        del clusters[c1_key]\n",
        "        del clusters[c2_key]\n",
        "        for k in clusters:\n",
        "            del clusters[k][c1_key]\n",
        "            del clusters[k][c2_key]\n",
        "\n",
        "        # Add new cluster to matrix\n",
        "        for k, dist in new_row.items():\n",
        "            clusters[k][c_new_key] = dist\n",
        "        clusters[c_new_key] = new_row\n",
        "\n",
        "    # The last remaining key is the root of the tree\n",
        "    root_key = list(clusters.keys())[0]\n",
        "    return cluster_info[root_key]['newick'] + \";\"\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# --- Demonstration and Testing\n",
        "# -----------------------------------------------------------------\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"  Running Demonstration for Assignment 3 - Part B\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # --- Q3 Test: Multiple Sequence Problems ---\n",
        "    print(\"\\n--- Q3: Multiple Sequence Problems  ---\")\n",
        "    seqA = \"GATTACA\"\n",
        "    seqB = \"GCATGCU\" # Note 'U'\n",
        "    seqB = seqB.replace('U', 'T') # Clean data\n",
        "\n",
        "    # Q3a: Sequence Alignment\n",
        "    score, al1, al2 = align_global_needleman_wunsch(seqA, seqB)\n",
        "    print(f\"[Q3a] Global Alignment:\")\n",
        "    print(f\"  Score: {score}\")\n",
        "    print(f\"  Seq1:  {al1}\")\n",
        "    print(f\"  Seq2:  {al2}\")\n",
        "\n",
        "    # Q3b: Longest Common Subsequence\n",
        "    seqC = \"AGCAT\"\n",
        "    seqD = \"GAC\"\n",
        "    lcs_len, lcs_str = find_longest_common_subsequence(seqC, seqD)\n",
        "    print(f\"\\n[Q3b] Longest Common Subsequence (LCS):\")\n",
        "    print(f\"  SeqC: {seqC}, SeqD: {seqD}\")\n",
        "    print(f\"  LCS: '{lcs_str}' (Length: {lcs_len})\")\n",
        "\n",
        "    # Q3c: Evolutionary Distance\n",
        "    p_dist = calc_p_distance(al1, al2)\n",
        "    jc_dist = calc_jukes_cantor_distance(p_dist)\n",
        "    print(f\"\\n[Q3c] Evolutionary Distance:\")\n",
        "    print(f\"  p-distance: {p_dist:.4f}\")\n",
        "    print(f\"  Jukes-Cantor distance: {jc_dist:.4f}\")\n",
        "\n",
        "    # Q3d: Consensus Sequence\n",
        "    msa = [\"GATTACA\",\n",
        "           \"G-TTACA\",\n",
        "           \"GATT-CA\",\n",
        "           \"GCTT-CA\",\n",
        "           \"AATTACA\"]\n",
        "    consensus, profile = generate_consensus_from_alignment(msa)\n",
        "    print(f\"\\n[Q3d] Consensus Generation:\")\n",
        "    for s in msa:\n",
        "        print(f\"  {s}\")\n",
        "    print(f\"  Consensus: {consensus}\")\n",
        "    # print(f\"  Profile: {profile}\") # Uncomment for full profile\n",
        "\n",
        "\n",
        "    # --- Q4 Test: Advanced Pattern Analysis ---\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"\\n--- Q4: Advanced Pattern Analysis  ---\")\n",
        "\n",
        "    genome = \"GATTACAGATTACAGATTACA$\"\n",
        "\n",
        "    # Q4a: Suffix Array\n",
        "    sa = build_suffix_array(genome)\n",
        "    print(f\"[Q4a] Suffix Array (first 10): {sa[:10]}...\")\n",
        "    # [cite: 42]\n",
        "    pattern = \"GATTACA\"\n",
        "    hits = find_pattern_suffix_array(genome, pattern, sa)\n",
        "    print(f\"  Pattern '{pattern}' found at indices: {hits}\")\n",
        "    # [cite: 42]\n",
        "\n",
        "    # Q4b: Palindromes\n",
        "    pal_seq = \"AGAGAATTCGC\" # Contains EcoRI site GAATTC\n",
        "    palindromes = find_reverse_palindromes(pal_seq, min_len=6, max_len=6)\n",
        "    print(f\"\\n[Q4b] Reverse Palindromes:\")\n",
        "    print(f\"  Seq: {pal_seq}\")\n",
        "    for pos, length, sub in palindromes:\n",
        "        print(f\"  Found '{sub}' at pos {pos}\")\n",
        "\n",
        "    # Q4c: Assembly Simulation\n",
        "    reads = [\"GATTACAGAT\", \"AGATTACAGA\", \"TACAGATTAC\"]\n",
        "    contig = simulate_greedy_assembly(reads, min_overlap=8)\n",
        "    print(f\"\\n[Q4c] Greedy Assembly Simulation:\")\n",
        "    print(f\"  Reads: {reads}\")\n",
        "    print(f\"  Assembled Contig: {contig}\")\n",
        "\n",
        "    # Q4d: Phylogenetic Tree (UPGMA)\n",
        "    # Simple distance matrix\n",
        "    labels = [\"Human\", \"Chimp\", \"Gorilla\", \"Orangutan\"]\n",
        "    dist_mat = {\n",
        "        \"Human\": {\"Chimp\": 0.02, \"Gorilla\": 0.04, \"Orangutan\": 0.08},\n",
        "        \"Chimp\": {\"Human\": 0.02, \"Gorilla\": 0.05, \"Orangutan\": 0.09},\n",
        "        \"Gorilla\": {\"Human\": 0.04, \"Chimp\": 0.05, \"Orangutan\": 0.10},\n",
        "        \"Orangutan\": {\"Human\": 0.08, \"Chimp\": 0.09, \"Gorilla\": 0.10}\n",
        "    }\n",
        "    tree = build_upgma_tree(dist_mat, labels)\n",
        "    print(f\"\\n[Q4d] UPGMA Phylogenetic Tree:\")\n",
        "    print(f\"  Distance Matrix (Human-Chimp): {dist_mat['Human']['Chimp']}\")\n",
        "    print(f\"  Newick Tree: {tree}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"  Part B Demonstration Complete.\")\n",
        "    print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIfr-HXySGc3",
        "outputId": "cecef6bb-aaf5-4cb7-b5a0-2e7870a244fd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "  Running Demonstration for Assignment 3 - Part B\n",
            "============================================================\n",
            "\n",
            "--- Q3: Multiple Sequence Problems  ---\n",
            "[Q3a] Global Alignment:\n",
            "  Score: -1.0\n",
            "  Seq1:  GATTACA\n",
            "  Seq2:  GCATGCT\n",
            "\n",
            "[Q3b] Longest Common Subsequence (LCS):\n",
            "  SeqC: AGCAT, SeqD: GAC\n",
            "  LCS: 'GA' (Length: 2)\n",
            "\n",
            "[Q3c] Evolutionary Distance:\n",
            "  p-distance: 0.5714\n",
            "  Jukes-Cantor distance: 1.0763\n",
            "\n",
            "[Q3d] Consensus Generation:\n",
            "  GATTACA\n",
            "  G-TTACA\n",
            "  GATT-CA\n",
            "  GCTT-CA\n",
            "  AATTACA\n",
            "  Consensus: GATTACA\n",
            "\n",
            "============================================================\n",
            "\n",
            "--- Q4: Advanced Pattern Analysis  ---\n",
            "[Q4a] Suffix Array (first 10): [22, 21, 20, 18, 11, 4, 13, 6, 15, 8]...\n",
            "  Pattern 'GATTACA' found at indices: [0, 7, 14]\n",
            "\n",
            "[Q4b] Reverse Palindromes:\n",
            "  Seq: AGAGAATTCGC\n",
            "  Found 'GAATTC' at pos 3\n",
            "\n",
            "[Q4c] Greedy Assembly Simulation:\n",
            "  Reads: ['GATTACAGAT', 'AGATTACAGA', 'TACAGATTAC']\n",
            "  Assembled Contig: AGATTACAGAT\n",
            "\n",
            "[Q4d] UPGMA Phylogenetic Tree:\n",
            "  Distance Matrix (Human-Chimp): 0.02\n",
            "  Newick Tree: (Orangutan:0.0450,(Gorilla:0.0225,(Human:0.0100,Chimp:0.0100):0.0125):0.0225);\n",
            "\n",
            "============================================================\n",
            "  Part B Demonstration Complete.\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part C: Real-World Applications and Optimization (25 marks)**"
      ],
      "metadata": {
        "id": "y2bJJmJhTQ10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code for the Part C was done in VScode due to the command line interface question.Added the py file with txt files of result."
      ],
      "metadata": {
        "id": "Q01XgWYbcGY1"
      }
    }
  ]
}